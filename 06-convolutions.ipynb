{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Housekeeping\n\n"]},{"cell_type":"markdown","metadata":{},"source":["-   Some references for today\n    -   <span class=\"underline\">[Official pytorch tutorials](https://pytorch.org/tutorials/)</span>\n    -   <span class=\"underline\">[Pytorch tutorials by yunjey, from beginning to advanced](https://github.com/yunjey/pytorch-tutorial)</span>\n    -   <span class=\"underline\">[Deep Learning Book on ConvNets](https://www.deeplearningbook.org/contents/convnets.html)</span>\n    -   <span class=\"underline\">[MIT Intro to Deep Learning Lecture on ConvNets](https://www.youtube.com/watch?v=iaSUYvmCekI&list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI&index=3)</span>\n-   Today's url \n    -   [https://git.io/2020deep06](https://git.io/2020deep06)\n-   Email if you need help with any of this!\n\n"]},{"cell_type":"markdown","metadata":{},"source":["## What do we want a neural network to do?\n\n"]},{"cell_type":"markdown","metadata":{},"source":["![img](mnist_digits_examples.png)\n\n-   Thinking of our mnist dataset from last week and a 2 layer hidden\n    network, we might expect/want the network to piece together a\n    structure like this\n    -   In the first layer, find loops and lines in the various parts of the image\n    -   In the second layer, see if they combine together (fire at the\n        same time), in certain combinations\n-   In the example, theres a \"loop\" a the top connected to a \"line\" for\n    9, a similar \"loop\" at the bottom but connect to a \"hook\", a \"line\"\n    similar to that in 9 for the 1, and some \"hooks\" for the 3\n-   Is the network structured this way? I.e. are there \"loop\", \"hook\",\n    \"line\" finders\n\nidea from <span class=\"underline\">[3blue1brown on neural nets](https://www.youtube.com/watch?v=aircAruvnKk)</span>\n\n"]},{"cell_type":"markdown","metadata":{},"source":["## What does it really do?\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### :BMCOL:\n\n"]},{"cell_type":"markdown","metadata":{},"source":["![img](filter_weights_mnist.png)\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### :BMCOL:\n\n"]},{"cell_type":"markdown","metadata":{},"source":["-   Plot of the weights from each pixel connection in the first layer of\n    a 2 hidden layer model from MNIST (red means this pixel should be\n    fired to fire this node, blue means that if the pixel is fired the\n    node is suppressed)\n-   Maybe recognizable structures, but lots of fairly random swirls\n-   One issue with the network is that structures like loops and lines\n    need to be fit, but these can appear in different places on the image\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### :B_ignoreheading:\n\n"]},{"cell_type":"markdown","metadata":{},"source":["-   So, we don't have a \"loop finder\" node, we need to find loops in any\n    of many different points on the image, leading to a jumble of weights\n-   Can we change our network so \"loop finder\" structures are possible?\n\n"]},{"cell_type":"markdown","metadata":{},"source":["## Convolutional Layers\n\n"]},{"cell_type":"markdown","metadata":{},"source":["-   A convolution layer is a connection between one layer and the next\n    in a NN with a very specific structure:\n    -   Typically, it works with a 3d input like an image: channels (red,\n        green, blue), width, height\n    -   It contains a **kernel** or **filter**, which is a 3d block sized\n        $channel \\times n \\times m$, $n$ and $m$ are user-specifed, with\n        each element of the block a weight to be set in training\n    -   The outputs consists of all $n \\times m$ *convolutions* of the\n        filter with the image, creating a new one-channel image\n        -   Discrete convolution, meaning each element of the kernel is\n            multiplied with a pixel in (one channel of) the image, and all\n            are summed together\n    -   The output of the filter is passed through an activation function,\n        the same as the usual fully-connected layer\n-   A single convolutional layer generally consists of many\n    convolutional filters, each filter giving one layer in the output\n-   Networks with convolutional layers are Convolutional Neural\n    Networks: CNN\n\n"]},{"cell_type":"markdown","metadata":{},"source":["## Convolutional Filters In Pictures\n\n"]},{"cell_type":"markdown","metadata":{},"source":["![img](conv_schem-0.png)\n\n![img](conv_schem-1.png)\n\n![img](conv_schem-2.png)\n\n![img](conv_schem-3.png)\n\n![img](conv_schem-4.png)\n\n![img](conv_schem-5.png)\n\n![img](conv_schem-6.png)\n\n![img](conv_schem-7.png)\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### :BMCOL:\n\n"]},{"cell_type":"markdown","metadata":{},"source":["![img](conv_schem-8.png)\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### :BMCOL:\n\n"]},{"cell_type":"markdown","metadata":{},"source":["-   A [filter](olive) sliding over the [image](MyGreen) builds up the [output layer](red), each\n    output is sum of filter elements multiplied by image pixels\n-   The same filter is used for each pixel, the weights are learnt\n    during training (as well as an output bias)\n\n"]},{"cell_type":"markdown","metadata":{},"source":["## Example Filter\n\n"]},{"cell_type":"markdown","metadata":{},"source":["![img](edge_detection.png)\n\n-   As an example, here is a 3x3 filter for detecting vertical edges\n-   The opposing plus and minus sides cancel in a [block of color](MyGreen)\n-   [At an edge](red), the filter is either highly positive (white to left of\n    edge), or negative (white to right of edge)\n-   What would a horizontal edge detector look like?\n\nAndrew Ng lecture by way of [https://kharshit.github.io/blog/2018/12/14/filters-in-convolutional-neural-networks](https://kharshit.github.io/blog/2018/12/14/filters-in-convolutional-neural-networks)\n\n"]},{"cell_type":"markdown","metadata":{},"source":["## Multiple Filter Outputs\n\n"]},{"cell_type":"markdown","metadata":{},"source":["![img](multiple_layers.png)\n\n-   When multiple filters are used in a single layer, they have the same\n    width and height, so they can be put together in a single output as\n    $channels \\times width \\times height$\n-   This is exactly the image structure which was the input to the\n    network\n-   This means this convolutional structure can be used several times in\n    series\n    -   Each successive layer effectively sees a larger part of the image,\n        since each pixel in the output of one layer is from several pixels\n-   The image shows that a 3-channel input needs filters with a 3x3x3\n    block, and 2 filters produce a 2 channel output\n\n"]},{"cell_type":"markdown","metadata":{},"source":["## Filters Over Several Input Layers\n\n"]},{"cell_type":"markdown","metadata":{},"source":["![img](features.png)\n\n-   Convolutional layers are typically built up one after the other\n-   The idea is that features get *built up*, at low levels, you might\n    have edge detectors, later layers use these edges to build up\n    structure, and by high levels recognizable objects are being\n    searched for\n    -   These images are made by doing reverse gradient descent on the\n        network, i.e. updating the image pixels themselves, trying to make\n        the image \"light up\" (set node output high) a particular node\n-   Networks these days can contain *hundreds* of these layers\n    -   This is the meaning of *deep* in deep learning\n\nImage from [https://twopointseven.github.io/2017-10-29/cnn/](https://twopointseven.github.io/2017-10-29/cnn/)\n\n"]},{"cell_type":"markdown","metadata":{},"source":["## Strides and Padding\n\n"]},{"cell_type":"markdown","metadata":{},"source":["![img](strides-padding.jpg)\n\n-   When sliding across the image, you can move the filter more than 1\n    pixel at a time, this is the *stride*\n    -   By default its just 1, ie sliding the image\n-   The filter will reduce the size of the image (can only fit in so\n    many 3x3 blocks), you can *pad* the image (with zeros, or copying\n    the outer variables) to keep the outputs the same size\n-   Can also use different strides or pads in the vertical and\n    horizontal directions\n\n"]},{"cell_type":"markdown","metadata":{},"source":["## Pooling Layers\n\n"]},{"cell_type":"markdown","metadata":{},"source":["-   We may want to reduce the size of the images flowing through the\n    network for computational and conceptual efficiency reasons\n    -   As we add filters, we should be building up higher level features,\n        which are less localized on the image\n    -   Another way to say this is we want to *downsample* the image\n-   We can reduce the image through *pooling*, applying an operation on\n    each $n \\times n$ patch of the image (leaping **not** sliding)\n-   A typical use is max pooling, we could find the maximum of each\n    patch of the image\n-   Here, we apply a $2 \\times 2$ max pooling to reduce a $4 \\times 4$\n    matrix to $2 \\times 2$\n-   Another typical operation is to take the *average* of each patch\n\n"]},{"cell_type":"markdown","metadata":{},"source":["## Structuring a Network with Convolutional Layers\n\n"]},{"cell_type":"markdown","metadata":{},"source":["![img](network.png)\n\n-   The basic CNN consists of several convolutional layers, followed by\n    \"squashing\" the output of the last convolution into a regular 1d\n    node structure, after which the fully connected layers of a normal\n    NN can be used\n-   So the idea is, the convolutional layers search for particular high\n    level \"features\", then the output is decided by which features do or\n    do not exist in the network\n\nImage from [https://twopointseven.github.io/2017-10-29/cnn/](https://twopointseven.github.io/2017-10-29/cnn/)\n\n"]},{"cell_type":"markdown","metadata":{},"source":["## Some Benefits of The Convolutional Neural Network\n\n"]},{"cell_type":"markdown","metadata":{},"source":["-   Fewer parameters than a fully connected network\n    -   Parameters for a cxhxw image fully connected to n nodes: cxhxwxn + n\n    -   Parameters for a cxhxw image convolutional to n mxm filters (no padding/stride 1): cxmxmxn + n\n    -   If our filter size is smaller than the image, much, much fewer\n        parameters, and independent of input height, width\n    -   Fewer parameters is better for overtraining\n-   The sliding connections mean the network can learn features independent of position\n    -   A fully connected layer would need to learn what a 'hand' or an\n        'eye' looks like independently everywhere it could be in the image\n    -   This *parameter sharing* between parts of the image means that the\n        network can learn more robust features\n-   We have developed a structure which could possibly be our \"loop finder\"\n\n"]},{"cell_type":"markdown","metadata":{},"source":["## Convolutional Filters in pytorch\n\n"]},{"cell_type":"markdown","metadata":{},"source":["-   torch.nn.Conv2d provides a convolutional filter, you tell it:\n    -   The number of input channels\n    -   The number of output channels\n    -   The size of the filter (can be a number for nxn or a 2-tuple for nxm)\n    -   Optionally, you can change the stride and the padding\n-   The filters take in tensors of rank 4, with shape: `(number of\n      images, number of channels, height of image, width of image)`\n    (pytorch always assumes you're processing multiple images)\n-   The output is also a rank 4 tensor, with the number of output\n    channels changed, and the height and width can be expanded or\n    contracted by changing the stride and padding\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"tensor([[[[-0.2259]],\n         [[-0.1640]]]], grad_fn=<MkldnnConvolutionBackward>)"}],"source":["# convolutional filter from 1 -> 2 channels, with 3x3 filter\nconv_filter = torch.nn.Conv2d(1,2,3)\nconv_filter(torch.tensor([ [ [[1,1,1],[1,1,1],[1,1,1.]] ] ]))"]},{"cell_type":"markdown","metadata":{},"source":["## Pooling Layers in pytorch\n\n"]},{"cell_type":"markdown","metadata":{},"source":["-   Similar to Conv2d, there is `torch.nn.MaxPool2d` and `torch.nn.AvgPool2d` to max and average pooling respectively,\n-   They only need to be given the filter size, and have similar\n    input/output shapes (rank-4 tensors everywhere)\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"tensor([[[[2., 4.],\n          [5., 7.]]]])"}],"source":["pool = torch.nn.MaxPool2d(2)\npool(torch.tensor([ [ [[1,2,3,4],[1,1,1,1],\n                       [1,1,1.,1],[4,5,6,7]] ] ]))"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"tensor([[[[1.2500, 2.2500],\n          [2.7500, 3.7500]]]])"}],"source":["pool = torch.nn.AvgPool2d(2)\npool(torch.tensor([ [ [[1,2,3,4],[1,1,1,1],\n                       [1,1,1.,1],[4,5,6,7]] ] ]))"]},{"cell_type":"markdown","metadata":{},"source":["## Building a Network\n\n"]},{"cell_type":"markdown","metadata":{},"source":["-   Networks will at some point need to go from processing 2d images\n    with multiple channels, to a discrete probability distribution (if\n    we are making a classifier)\n-   You can insert a `view` into the `forward` function to adjust the\n    output nodes into a 1d line (-1 at the front so it automatically\n    sizes to any number of images in the input)\n-   Here is a simple CNN for MNIST with 1 convolutional layer, which is\n    reshaped and then connected to the 10 category output layer\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["class SimpleCNN(torch.nn.Module):\n  def __init__(self):\n    super(SimpleCNN, self).__init__()\n    self.conv = torch.nn.Conv2d(1,6,5) # 5x5 filter, no padding\n    self.fc = torch.nn.Linear(6*24*24,10)\n  def forward(self, x):\n    x = torch.tanh(self.conv(x))\n    x = self.fc(x.view(-1, 6*24*24))\n    return x"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"SimpleCNN(\n  (conv): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n  (fc): Linear(in_features=3456, out_features=10, bias=True)\n)\n[150, 6, 34560, 10]"}],"source":["sum(p.numel() for p in net.parameters())\nprint(net)\nprint(list(p.numel() for p in net.parameters()))"]},{"cell_type":"markdown","metadata":{},"source":["## Exercises\n\n"]},{"cell_type":"markdown","metadata":{},"source":["We will train a few convolutional networks in pytorch.\n\nTraining a convnet. \n\nLeNet 5 was the first convolutional neural network, created all the\nway back in the dark ages of 1998.  It was designed to perform\nhandwriting digit recognition for the US post office. MNIST was in\nfact created to show that CNN were superior for this task.\n\nThe structure of LeNet-5 is shown below:\n\n![img](lenet5.png)\n\nThis is interpreted as the input image starting at the bottom, and it\ngets processed by the succsessive layers with the output of one layer\nflowing to the next. Note that the diagram indicates whether the\nconv2d layers are zero padded to keep the output the same size as the\ninput, and the number of output channels, and filter size are\nindicated in the diagram. (Also, technically, LeNet-5 has some\nslightly non-standard layer features that never caught / were never\nfound useful in the deep learning community, so this is a simplified\nversion). The activation functions after the layers are tanh, insert\nan activation after the pooling and fully connected layers.\n\nHere is the code to load MNIST (don't forget to change the directory!).\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import torch\nimport matplotlib.pyplot as plt\nimport torchvision\nimport torchvision.transforms as transforms\n\ntrans = transforms.ToTensor()\ntrain = torchvision.datasets.MNIST('~/data/torchvision', train=True, download=True, transform=trans)\ntest = torchvision.datasets.MNIST('~/data/torchvision', train=False, download=True, transform=trans)\ntrainloader = torch.utils.data.DataLoader(train, batch_size=64)\ntestloader = torch.utils.data.DataLoader(test, batch_size=64)"]},{"cell_type":"markdown","metadata":{},"source":["Based on the structure given above, create LeNet-5 in pytorch, and\ntrain it on the MNIST dataset. The usual cross-entropy loss can be\nused (convolutional filters can be backpropagated through, as a fully\nconnected layer can) and the usual optimizers we've seen in previous\nweeks are also usable with CNNs with no new surprises. What is the\nbest loss/accuracy you can achieve?  ([LeCun was able to get to 99%](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf)\nusing some tricks we'll see later in the course)\n\nKeep track of the test/train curve (sample several times an epoch),\nand check for overtraining, and show the accuracy of your best\nmodel.\n\nIf training is taking to long, check you're running on the GPU (!).\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{},"source":["Similar to MNIST above, `torchvision.datasets.FashionMNIST` can\ndownload the Fashion-MNIST, which is a more difficult version of\nMNIST. Instead of handwritten numbers, you have black and white images\nof various items of clothing, which you should train a classifier to\ndistinguish.\n\nCopying the MNIST download cell above, download the fashion MNIST\ndata, and then, display a few images from the dataset using\nmatplotlib. Also, check `train.classes` to see what the different\ncategories in the Fashion-MNIST dataset are. Can you recognize the\ndifferent categories in your images?\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{},"source":["Write and train a network to distinguish clothing categories from the\nFashion MNIST data. Try to get the best accuracy on the test set you\ncan. Since we have more compute power than LeCun could have dreamed of\nin the 90s (GPUs weren't really feasible to train on until 2012), you\ncan expand and adapt the LeNet-5 network as you like. You can add more\nconvolutional filters, and change the filter sizes, increase or\ndecrease the hidden layers, try different activation functions, and so\non. As you make changes, run and see how the training time and overall\naccuracy changes. Train and compare at least 2 networks.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":[""]}],"metadata":{"org":null,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}
